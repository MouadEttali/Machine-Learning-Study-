{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Methodologie_NAGC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Projet méthodologie de recherche : Non-linear Attributed Graph Clustering by Symmetric NMF with PU Learning**\n",
        "\n",
        "Réalisé par : Oubenyahya Sanae & Hilal Chaimae & Ettali Mouad & Ouazzani Chahdi Kenza"
      ],
      "metadata": {
        "id": "hx23aAa6fp0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def VU_init(A,k1,k2,flag,data):\n",
        "    n = A.shape[0]\n",
        "    m = A.shape[1]\n",
        "    if flag == 0:\n",
        "        U = np.random.random((n,k1))\n",
        "        V = np.random.random((m,k2))\n",
        "    else:\n",
        "        infile = 'initialize/'+data+'_U_'+str(k1)+'.csv'\n",
        "        U = []\n",
        "        with open(infile) as f:\n",
        "            while True:\n",
        "                row = []\n",
        "                line = f.readline()\n",
        "                if line == '':\n",
        "                    break\n",
        "                line = line.rstrip('\\n')\n",
        "                tmp = line.split(',')\n",
        "                for i in tmp:\n",
        "                    row.append(float(i))\n",
        "                U.append(row)\n",
        "        infile = 'initialize/'+data+'_V_'+str(k2)+'.csv'\n",
        "        V = []\n",
        "        with open(infile) as f:\n",
        "            while True:\n",
        "                row = []\n",
        "                line = f.readline()\n",
        "                if line == '':\n",
        "                    break\n",
        "                line = line.rstrip('\\n')\n",
        "                tmp = line.split(',')\n",
        "                for i in tmp:\n",
        "                    row.append(float(i))\n",
        "                V.append(row)\n",
        "        U = np.array(U)\n",
        "        V = np.array(V).transpose()\n",
        "    return U,V"
      ],
      "metadata": {
        "id": "PnetW4T0BVBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwBZfvJmBAfO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "NJNMF with PU-learning is described in this program.\n",
        "We recommend using kmeans initialization (init=1).\n",
        "In that case, you run kmeans_init.py in advance.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import sys\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "# import networkx as nx\n",
        "# from matplotlib import pyplot\n",
        "# max_iter=100 #number of iterations\n",
        "b=0.0   #bias for sigmoid function\n",
        "\n",
        "class NAGC:\n",
        "    def __init__(self,k1,k2,lam,S,W,X,data,init=1,rho=0.5,max_iter=100):\n",
        "        node_size = S.shape[0]\n",
        "        att_size = X.shape[1]\n",
        "        #H is a matrix between topic1 and topic2\n",
        "        H = np.random.random((k1,k2))\n",
        "        #create initialized matrices\n",
        "        U,V = VU_init.VU_init(X,k1,k2,init,data)\n",
        "        # a = (1.0+rho)/2\n",
        "        # W = np.abs(S_ori-1)\n",
        "        W_ = 1-W\n",
        "        # general params\n",
        "        self.S = S\n",
        "        self.W = W\n",
        "        self.W_ = W_\n",
        "        self.X = X\n",
        "        self.max_iter = max_iter\n",
        "        self.U = U\n",
        "        self.H = H\n",
        "        self.V = V\n",
        "        self.lam = lam\n",
        "        self.rho = rho\n",
        "    def fit_predict(self):\n",
        "        def update_U(S,W,W_,X,lam,U,H,V,rho):\n",
        "            fUH = sigmoid(U.dot(H))\n",
        "            fdUH = dif_sig(U.dot(H))\n",
        "            UUT = U.dot(U.transpose())\n",
        "            U = U*((rho*2.0)*S.dot(U)+(lam*X.dot(V) * fdUH).dot(H.transpose()))/((2.0*rho)*(UUT*W).dot(U)+(2.0*(1.0-rho))*(UUT*W_).dot(U)+(lam*fUH.dot(V.transpose().dot(V)) * fdUH).dot(H.transpose()))\n",
        "            #V = V*((a*2.0)*S.dot(V)+(lam*A.dot(U) * fdVT).dot(T.transpose()))/(((2.0*a)*(VVT*S_ori)+(2.0*(1.0-a))*(VVT*S_)).dot(V)+(lam*fVT.dot(U.transpose().dot(U)) * fdVT).dot(T.transpose()))\n",
        "            return U\n",
        "        def update_U_woPU(S,X,lam,U,H,V):\n",
        "            fUH = sigmoid(U.dot(H))\n",
        "            fdUH = dif_sig(U.dot(H))\n",
        "            U = U*(2.0*S.dot(U)+(lam*X.dot(V) * fdUH).dot(H.transpose()))/(2*U.dot(U.transpose().dot(U))+(lam*fUH.dot(V.transpose().dot(V)) * fdUH).dot(H.transpose()))\n",
        "            return U\n",
        "        def update_V(S,X,U,H,V):\n",
        "            # fVT = sigmoid(V.dot(T))\n",
        "            return V*(X.transpose().dot(sigmoid(U.dot(H))))/(V.dot(sigmoid(U.dot(H).transpose()).dot(sigmoid(U.dot(H)))))\n",
        "        def update_H(S,X,U,H,V):\n",
        "            fUH = sigmoid(U.dot(H))\n",
        "            fdUH = dif_sig(U.dot(H))\n",
        "            H = H*(U.transpose().dot(fdUH*(X.dot(V))))/(U.transpose().dot((fdUH*fUH).dot(V.transpose().dot(V))))\n",
        "            return H\n",
        "        def removing_nan(mat):\n",
        "            nan_list = np.argwhere(np.isnan(mat))\n",
        "            for i in nan_list:\n",
        "                mat[i[0],i[1]]=sys.float_info.epsilon\n",
        "            return mat\n",
        "        def sigmoid(x):\n",
        "            return 1.0 / (1.0 + np.exp(-(x-b)))\n",
        "        def dif_sig(x):\n",
        "            return 1.0 * np.exp(-(x-b)) / pow(1.0+np.exp(-(x-b)),2)\n",
        "\n",
        "        start = time.time() #memo start time\n",
        "        #learning step\n",
        "        count = 0\n",
        "        while 1:\n",
        "            count += 1\n",
        "            # print loss_function(S,V,U,Z,A,T,lam)\n",
        "            if self.rho == 0.5:\n",
        "                self.U = removing_nan(update_U_woPU(self.S,self.X,self.lam,self.U,self.H,self.V))\n",
        "            else:\n",
        "                self.U = removing_nan(update_U(self.S,self.W,self.W_,self.X,self.lam,self.U,self.H,self.V,self.rho))\n",
        "            self.V = removing_nan(update_V(self.S,self.X,self.U,self.H,self.V))\n",
        "            self.H = removing_nan(update_H(self.S,self.X,self.U,self.H,self.V))\n",
        "            if count>=self.max_iter:\n",
        "                break\n",
        "        elapsed_time = time.time() - start  #measure elapsed time\n",
        "        print ((\"optimizing_time:{0}\".format(elapsed_time)) + \"[sec]\")\n",
        "        return self.U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This program build adjacency matrix and attribute matrix from graph data.\n",
        "#This program can deal with two format(.mat and .cite&.content)\n",
        "#Outputs are blow\n",
        "'''\n",
        "S : adjacency matrix with prepocess\n",
        "S_ori : original adjacency matrix\n",
        "A : attribute matrix with preprocess\n",
        "clus : true cluster of nodes\n",
        "flag : with ground truth or without\n",
        "A_ori : original attribute matrix\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import scipy.io\n",
        "\n",
        "def build_graph(path): # switch for two form of file\n",
        "    if \"mat\" in path:\n",
        "        print (\"mat\")\n",
        "        S,S_ori,A,clus,A_ori = for_mat(path)\n",
        "        flag=1\n",
        "        if clus==[]:\n",
        "            flag=0\n",
        "        return S,S_ori,A,clus,flag,A_ori\n",
        "    else:\n",
        "        print (\"cite\")\n",
        "        S,S_ori,A,clus,A_ori = for_cites_contents(path)\n",
        "        return S,S_ori,A,clus,1,A_ori\n",
        "\n",
        "# def for_mat(path):\n",
        "#     matdata = scipy.io.loadmat(path)\n",
        "#     a = matdata[\"A\"]\n",
        "#     f = matdata[\"F\"]\n",
        "#     node_size = a.shape[0]\n",
        "#     att_size = f.shape[1]\n",
        "#     # S = lil_matrix((node_size,node_size))\n",
        "#     S = np.zeros((node_size,node_size))\n",
        "#     A = np.zeros((node_size,att_size))\n",
        "#     #fill the adjacency matrix and attribute matrix\n",
        "#     nonzeros = a.nonzero()\n",
        "#     print (\"no.nodes: \" + str(node_size))\n",
        "#     print (\"no.attributes: \" + str(att_size))\n",
        "#     edgecount=0\n",
        "#     for i in range(len(nonzeros[0])):\n",
        "#         S[nonzeros[0][i],nonzeros[1][i]] = 1\n",
        "#         if nonzeros[0][i]<=nonzeros[1][i]:\n",
        "#             edgecount+=1\n",
        "#     print (\"no.edges: \" + str(edgecount))\n",
        "#     # S=S.tocsr()\n",
        "#     nonzeros = f.nonzero()\n",
        "#     for i in range(len(nonzeros[0])):\n",
        "#         A[nonzeros[0][i]][nonzeros[1][i]] = f[nonzeros[0][i],nonzeros[1][i]]\n",
        "#     # print A\n",
        "#     # reg=1\n",
        "#     # A_copy = copy.deepcopy(A)\n",
        "#     # if reg == 1:\n",
        "#     #     for i in range(A_copy.shape[1]):\n",
        "#     #         max_att = max(A_copy[:,i])\n",
        "#     #         if max_att != 0:\n",
        "#     #             A_copy[:,i] = A_copy[:,i]/max_att*100\n",
        "#     S_pre,A_pre=preprocess(S,A)\n",
        "#     return S_pre,S,A_pre,[],A #scaling S and A\n",
        "\n",
        "def for_mat(path):\n",
        "    mat_contents = scipy.io.loadmat(path)\n",
        "    G = mat_contents[\"Network\"]\n",
        "    X = mat_contents[\"Attributes\"]\n",
        "    Label = list(map(int,mat_contents[\"Label\"]))\n",
        "    node_size = G.shape[0]\n",
        "    att_size = X.shape[1]\n",
        "    # S = lil_matrix((node_size,node_size))\n",
        "    # S = np.zeros((node_size,node_size))\n",
        "    # A = np.zeros((node_size,att_size))\n",
        "    S = np.zeros((node_size,node_size))\n",
        "    A = X.toarray()\n",
        "    #fill the adjacency matrix and attribute matrix\n",
        "    nonzeros = G.nonzero()\n",
        "    print (\"no.nodes: \" + str(node_size))\n",
        "    print (\"no.attributes: \" + str(att_size))\n",
        "    edgecount=0\n",
        "    for i in range(len(nonzeros[0])):\n",
        "        S[nonzeros[0][i],nonzeros[1][i]] = 1\n",
        "        S[nonzeros[1][i],nonzeros[0][i]] = 1\n",
        "    # erase diagonal element\n",
        "    diag = 0\n",
        "    for i in range(node_size):\n",
        "        diag += S[i,i]\n",
        "    nonzeros = S.nonzero()\n",
        "    edge_count = int((len(nonzeros[0])+diag)/2)\n",
        "    print (\"number of edges : \" + str(edge_count))\n",
        "\n",
        "    S_pre,A_pre=preprocess(S,A)\n",
        "    return S_pre,S,A_pre,Label,A #scaling S and A\n",
        "\n",
        "\n",
        "def for_cites_contents(path):\n",
        "    node={}\n",
        "    counter=0\n",
        "    att_list=[]\n",
        "    clus=[]\n",
        "############ download atttributes #################\n",
        "    infiles = glob.glob(path+'/*.content')\n",
        "    for infile in infiles:\n",
        "        with open(infile) as f:\n",
        "            while True:\n",
        "                line = f.readline()\n",
        "                if line == '':\n",
        "                    break\n",
        "                tmp = line.split(\"\\t\")\n",
        "                node[tmp[0]]=counter\n",
        "                counter+=1\n",
        "                att_list.append((tmp[1:-1]))\n",
        "                clus.append(tmp[-1].replace(\"\\n\",\"\"))\n",
        "    print (\"number of nodes : \" + str(len(node)))\n",
        "    print (\"number of attributes : \" + str(len(att_list[0])))\n",
        "    # print (clus)\n",
        "############ download edges #################\n",
        "    edges=[]\n",
        "    infiles = glob.glob(path+'/*.cites')\n",
        "    for infile in infiles:\n",
        "        with open(infile) as f:\n",
        "            while True:\n",
        "                line = f.readline()\n",
        "                if line == '':\n",
        "                    break\n",
        "                line = line.replace(\"\\n\",\"\")\n",
        "                tmp = line.split()\n",
        "                if tmp[0] in node and tmp[1] in node:\n",
        "                    ind0 = node[tmp[0]]\n",
        "                    ind1 = node[tmp[1]]\n",
        "                    edges.append((ind0,ind1))\n",
        "    node_size = len(node)\n",
        "    att_size = len(att_list[0])\n",
        "    # S = lil_matrix((node_size,node_size))\n",
        "    S = np.zeros((node_size,node_size))\n",
        "    # A = lil_matrix((node_size,att_size))\n",
        "    A = np.zeros((node_size,att_size))\n",
        "    for i in range(len(edges)):\n",
        "        S[edges[i][0],edges[i][1]] = 1\n",
        "        S[edges[i][1],edges[i][0]] = 1\n",
        "    # erase diagonal element\n",
        "    diag = 0\n",
        "    for i in range(node_size):\n",
        "        diag += S[i,i]\n",
        "    nonzeros = S.nonzero()\n",
        "    edge_count = int((len(nonzeros[0])+diag)/2)\n",
        "    print (\"number of edges : \" + str(edge_count))\n",
        "    # S=S.tocsr()\n",
        "    for i in range(len(att_list)):\n",
        "        for j in range(len(att_list[0])):\n",
        "            A[i,j] = float(att_list[i][j])\n",
        "    # A=A.tocsr()\n",
        "\n",
        "    S_pre,A_pre=preprocess(S,A)\n",
        "    return S_pre,S,A_pre,clus,A #scaling S and A\n",
        "\n",
        "def preprocess(S,A):\n",
        "    # initialization in the paper(JWNMF)\n",
        "    # S = S / S.sum()\n",
        "    # A = A / A.sum()\n",
        "\n",
        "    # initialization based on size of S\n",
        "    # A = A * S.sum() / A.sum()\n",
        "    S = S * A.sum() / S.sum()\n",
        "    return S,A"
      ],
      "metadata": {
        "id": "s_JMFgeDBaX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#this program include calculater of modularity, entropy, NMI and ARI\n",
        "import sys\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.io\n",
        "from scipy.sparse import lil_matrix, csr_matrix\n",
        "import math\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "\n",
        "def evaluate(mod,ent,spl,nmi,ari,true_clus,clus,pred,S,A,ent_flag,with_gt):\n",
        "    k = len(clus)\n",
        "    mod.append(cal_modularity(clus,S,k))\n",
        "    if ent_flag == 0:\n",
        "        ent.append(cal_entropy(clus,A,k))\n",
        "    elif ent_flag == 1:\n",
        "        ent.append(cal_dif_entropy(clus,A_copy,k))\n",
        "        spl.append(split_entropy(clus,A_ori,k))\n",
        "    if with_gt >= 3:\n",
        "        nmi.append(cal_nmi(true_clus,clus))\n",
        "        ari.append(ARI(true_clus,pred))\n",
        "    return mod,ent,spl,nmi,ari\n",
        "#this function calculates modularity from clustering result\n",
        "def cal_modularity(clus,S,k):\n",
        "    a = [0]*k\n",
        "    e = np.zeros((k,k))\n",
        "    for i in range(k):\n",
        "        for j in range(k):\n",
        "            for l in clus[i]:\n",
        "                for m in clus[j]:\n",
        "                    e[i][j]+=float(S[l,m])\n",
        "    # regularize e on the sum of S\n",
        "    e = e / S.sum()\n",
        "    for i in range(k):\n",
        "        a[i] = sum(e[i][:])\n",
        "    Q=0\n",
        "    for i in range(k):\n",
        "        Q+=e[i][i]-a[i]*a[i]\n",
        "    return Q\n",
        "\n",
        "def cal_entropy(clus,A,k):\n",
        "    E=0\n",
        "    for t in range(A.shape[1]):\n",
        "        for j in range(k):\n",
        "            if len(clus[j])==0:\n",
        "                continue\n",
        "            att=0\n",
        "            for n in clus[j]:\n",
        "                if A[n,t]==0:\n",
        "                    att+=1\n",
        "            pk=[1.0*att/len(clus[j]),1.0-1.0*att/len(clus[j])]\n",
        "            # print pk\n",
        "            E += len(clus[j]) * scipy.stats.entropy(pk)\n",
        "    return E/A.shape[0]/A.shape[1]\n",
        "\n",
        "def cal_nmi(true_clus, clus):\n",
        "    t_label_list = list(set(true_clus))\n",
        "    n_h=[]\n",
        "    for i in range(len(t_label_list)):\n",
        "        n_h.append(true_clus.count(i))\n",
        "    t_clus_ind = [[] for i in range(len(n_h))]\n",
        "    for i in range(len(true_clus)):\n",
        "        t_clus_ind[true_clus[i]].append(i)\n",
        "    n_l = []\n",
        "    for i in range(len(clus)):\n",
        "        n_l.append(len(clus[i]))\n",
        "\n",
        "########### print No. labels ###############\n",
        "    print(\"estimate_No.label : [ \",end=\"\")\n",
        "    for i in range(len(n_l)):\n",
        "        print(str(n_l[i])+\" \",end=\"\")\n",
        "    print(']')\n",
        "#############################################\n",
        "    n=sum(n_l)\n",
        "    nmi = 0\n",
        "    for h in range(len(n_h)):\n",
        "        for l in range(len(n_l)):\n",
        "            n_hl = len(list(set(t_clus_ind[h]) & set(clus[l])))\n",
        "            if n_hl != 0 and n_h[h] != 0 and n_l[l] != 0:\n",
        "                nmi += n_hl * math.log(1.0*n*n_hl/n_h[h]/n_l[l],2)\n",
        "    deno_h = 0\n",
        "    for h in range(len(n_h)):\n",
        "        if n_h[h] != 0:\n",
        "            deno_h += n_h[h] * math.log(1.0*n_h[h]/n,2)\n",
        "    deno_l = 0\n",
        "    for l in range(len(n_l)):\n",
        "        if n_l[l] != 0:\n",
        "            deno_l += n_l[l] * math.log(1.0*n_l[l]/n,2)\n",
        "    if deno_h*deno_l != 0.0:\n",
        "        nmi = nmi/math.sqrt(deno_h*deno_l)\n",
        "    else:\n",
        "        nmi = 0.0\n",
        "    return nmi\n",
        "\n",
        "def ARI(true_clus,clus):\n",
        "    return adjusted_rand_score(true_clus,clus)"
      ],
      "metadata": {
        "id": "h_YnjPYzBn6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def clustering(A,k):\n",
        "    kmeans_clus=[]\n",
        "    for i in range(k):\n",
        "        kmeans_clus.append([])\n",
        "############# regularization for input attributes\n",
        "    for i in range(A.shape[1]):\n",
        "        max_att = max(A[:,i])\n",
        "        if max_att != 0:\n",
        "            A[:,i] = A[:,i]/max_att\n",
        "############ Learing step of kmeans\n",
        "    kmeans = KMeans(n_clusters=k).fit(A)\n",
        "    pred = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    for i in range(len(pred)):\n",
        "        kmeans_clus[pred[i]].append(i)\n",
        "    return pred, centroids, kmeans_clus\n",
        "def initialize_U(A, centroids):\n",
        "    U = np.zeros((len(A),len(centroids)))\n",
        "    for i in range(U.shape[0]):\n",
        "        dis_list = []\n",
        "        for j in range(U.shape[1]):\n",
        "            dis_list.append(np.linalg.norm(A[i]-centroids[j]))\n",
        "        for j in range(U.shape[1]):\n",
        "            U[i,j]= (sum(dis_list)-dis_list[j]) / sum(dis_list)\n",
        "        U[i,:] = U[i,:] / sum(U[i,:])\n",
        "    return U\n",
        "\n",
        "def init_kmeans(k,data):\n",
        "    print(data)\n",
        "\n",
        "    path = \"data/\"+data\n",
        "    S, S_ori, A, true_clus, flag, A_ori = build_graph(path)\n",
        "    clus_list = list(set(true_clus))\n",
        "    print(clus_list)\n",
        "    clus_dic = {}\n",
        "    for i in range(len(clus_list)):\n",
        "        clus_dic[clus_list[i]]=i\n",
        "    for i in range(len(true_clus)):\n",
        "        true_clus[i] = clus_dic[true_clus[i]]\n",
        "\n",
        "    pred_l=[];cent_l=[];km_l=[]\n",
        "    mod=[];ent=[];nmi=[];ari=[]\n",
        "    for j in range(5):\n",
        "        pred, centroids, kmeans_clus = clustering(A_ori,k)\n",
        "        pred_l.append(pred)\n",
        "        cent_l.append(centroids)\n",
        "        km_l.append(kmeans_clus)\n",
        "        ari.append(evaluate.ARI(true_clus,pred))\n",
        "    ind=ari.index(sorted(ari)[2])\n",
        "    pred=pred_l[ind]\n",
        "    centroids=cent_l[ind]\n",
        "    kmeans_clus=km_l[ind]\n",
        "    U = initialize_U(A_ori, centroids)\n",
        "    f = open('initialize/'+data+'_U_'+str(k)+'.csv','w')\n",
        "    writer = csv.writer(f, lineterminator='\\n')\n",
        "    writer.writerows(U)\n",
        "    f.close()\n",
        "    f = open('initialize/'+data+'_V_'+str(k)+'.csv','w')\n",
        "    writer = csv.writer(f, lineterminator='\\n')\n",
        "    writer.writerows(centroids)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "5AvYZv8TC4oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**NAGC EXEMPLE**"
      ],
      "metadata": {
        "id": "lDNWnDBHC-jM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import NAGC_class\n",
        "import build_graph\n",
        "import evaluate\n",
        "import init_kmeans"
      ],
      "metadata": {
        "id": "VuAdRn86DDdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B86VtUv6ME7E",
        "outputId": "ae395056-65e9-4bd9-c1c4-ddf6ebedd3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=\"WebKB_univ\"\n",
        "#data=\"citeseer\"\n",
        "# data = \"polblog\"\n",
        "# data = \"cora\"\n",
        "data_path = \"/content/drive/MyDrive/data/data/\"+data\n",
        "S, W, X, true_clus, flag, A_ori = build_graph(data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjLGUXGYDFl0",
        "outputId": "9e0a447a-0208-4f60-d126-a34b74826e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cite\n",
            "number of nodes : 877\n",
            "number of attributes : 1703\n",
            "number of edges : 1480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k1=4 # number of clusters you want to extract\n",
        "k2=20 # number of topic2 "
      ],
      "metadata": {
        "id": "BDqydjmvDLnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Kmeans**"
      ],
      "metadata": {
        "id": "1ZIuCr77DL_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_kmeans(k1,data)"
      ],
      "metadata": {
        "id": "i82toz6bDPGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_kmeans(k2,data)"
      ],
      "metadata": {
        "id": "8n1_H8rKDRHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Modifier les parametres et executer le model**"
      ],
      "metadata": {
        "id": "O-a0u40aDTFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lam=0.0001 # balancing parameter between the topology and attributes\n",
        "rho=0.55\n",
        "iteration=100 # number of iterations\n",
        "init=1 # 0: random initialization, 1: kmeans initialization\n",
        "\n",
        "NAGC = NAGC_class.NAGC(k1,k2,lam,S,W,X,data,init,rho)\n",
        "U = NAGC.fit_predict()"
      ],
      "metadata": {
        "id": "2_aFCAlRDapL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Evaluation des résultats de clustering**"
      ],
      "metadata": {
        "id": "Yufc7SchDiPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = k1\n",
        "clus=[]\n",
        "for i in range(U.shape[1]):\n",
        "    clus.append([])\n",
        "pred = U.argmax(1)\n",
        "for i in range(U.shape[0]):\n",
        "    clus[pred[i]].append(i)\n",
        "print(\"modularity: \" + str(evaluate.cal_modularity(clus,S,k)))\n",
        "print(\"entropy: \" + str(evaluate.cal_entropy(clus,X,k)))\n",
        "if true_clus != []:\n",
        "    print(\"ARI: \" + str(evaluate.ARI(true_clus,pred)))"
      ],
      "metadata": {
        "id": "ly-q72x8DhEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
