# First project :dimensionality reduction and clustering
## comprised of 3 parts:
1. Tokenizing the text using :
    * Word2Vec
    * GloVe 
2. Clustering on orignal data (Classic 4 data which comprises of scientific articles' abstracts , and BBC data which contains headlines from the bbc news channel )
    * This is done on the data without any dimensionality reduction technique
    * We will be using this to benchmark against later methods
3. Using Tandem methods, this is done by using :
    * dimensionality reduction technique ( PCA , t-SNE , UMAP , Autoencoders)
    * Followed by a clustering technique ( Kmeans, Spherical Kmeans , factorial Kmeans , Hierarchal clustering(WARD , Complete , Linkage and Single Metrics), and HDBSCAN 
5. Clustering using combined methods ( where the dimension reduction and clustering are done at the same time ):
    * Reduced kmeans
    * Factorial kmeans
    * Deep Clustering Network
    * deep KMeans


# second project : Implementation of Non-linear Attributed Graph Clustering by Symmetric NMF with PU Learning
