# -*- coding: utf-8 -*-
"""Template Regression problem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hmMd1qxLR-o1hiznIOd1bxK6psFyXDpo

Take home assignment: predicting car prices

 **Mouad Et-tali**

the goal of this assignment is to predict the column *selling_price* based on the other explicative variables such as year, engine, car name...

# Used Installs
Category_encoders are used for encoding data into a form that can be processed by the machine learning models.
"""

pip install category-encoders

"""#Importing Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler 
from sklearn.model_selection import GridSearchCV, train_test_split, KFold, cross_val_score ,RandomizedSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, mean_absolute_error ,mean_absolute_percentage_error
from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.cross_decomposition import PLSRegression
from sklearn.tree import DecisionTreeRegressor
import missingno as msno
from sklearn.utils import shuffle 
from category_encoders import TargetEncoder, OneHotEncoder
import warnings
import colorsys
import math
warnings.filterwarnings("ignore")
sns.set(rc = {'figure.figsize': (20, 20)})
# %matplotlib inline

"""# Utility Functions

some prebuilt utility functions that I generally use in regression problems
"""

# Clipping outliers
  
def clipping_outliers(df, var):
  """
    Returns dataframe with all outliers removed


    @type  df: Pandas Dataframe
    @param df: Original Dataframe.

    @type  df: String
    @param df: name of a column in the dataframe.

    @rtype:   Pandas Dataframe
    @return:  Dataframe
  """
  IQR = df[var].quantile(0.75)-df[var].quantile(0.25)
  lower_bound = df[var].quantile(0.25) - 4*IQR
  upper_bound = df[var].quantile(0.75) + 4*IQR
  no_outliers = len(df[df[var]>upper_bound]) + len(df[df[var]<lower_bound])
  print('There are %i outliers in %s: %.3f%%' %(no_outliers, var, no_outliers/len(df)))
  df[var] = df[var].clip(lower_bound, upper_bound)
  return df

#Generating colors
def generate_colors(N):
  """
    Returns N generated colors

    This function is used in conjuction with try_regressors to
    plot different figures of models with different colors

    @type  N: integer
    @param N: Length of the list.
  
    @rtype:   Python List
    @return:  List of colors of length N
    """
  HSV_tuples = [(x*1.0/N, 0.5, 0.5) for x in range(N)] 
  return map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples)


def try_regressors(models, tuned = False):
  """
    this function:
    1- Trains the estimators inside the models list
    2- Creates a dataframe that includes the prediction of each model on the test data
    3- Populates the metrics' lists with their respective values 
    4- Plots data and the regression model fit for each model.

    @type  models: Python List
    @param models: List of the models we'll use for regression over the training data.
    @type  tuned: boolean
    @param tuned: Specifies if the models have been hyperparameter tuned or not
    @rtype:   Void
    @return:  None.
    """
  # Initiate lists
  error_mean_square = []
  error_mean_absolute = []
  root_mean_square = []
  r2_metric = []
  Model_name = []
  # Generate colors 
  colors = generate_colors(len(models))
  # Loop over the estimators
  for (model,color) in zip(models,colors):
    # Train the estimator
    model.fit(X_train_new, y_train)
    # Test the estimator
    y_predict = pd.DataFrame(model.predict(X_test_new), columns = ['Predicted Output'])
    # Populate the Lists which we'll use later to create a results dataframe
    error_mean_square.append(int(mean_squared_error(y_predict, y_test)))
    error_mean_absolute.append(int(mean_absolute_error(y_predict, y_test)))
    root_mean_square.append(math.sqrt(int(mean_squared_error(y_predict, y_test))))
    r2_metric.append(r2_score(y_predict, y_test))
    Model_name.append("tuned " + type(model).__name__ if tuned else type(model).__name__)
    # Plot the data and a linear regression fit for each estimator
    plt.figure(figsize = (19, 6))
    sns.regplot(data = pd.concat([y_predict, y_test.to_frame().reset_index(drop = True)],
                axis = 1, ignore_index = False),
                y = 'Predicted Output', x = 'selling_price',
                color = color, marker = 'o')
    plt.title("Comparision of predicted values and the actual values for "+type(model).__name__, fontsize = 13)
    plt.show()
    print('\n')
  results_data = {'Models': Model_name,
        'Mean Square Error': error_mean_square,
        'Root Mean Square Error' : root_mean_square,
        'Mean Absolute Error': error_mean_absolute,
        'R2 score':r2_metric,
        } 
  
  return results_data

def try_one_regressor(model, tuned = False):
  """
    this function:
    1- Trains the estimator 
    2- Creates a dataframe that includes the predictions of the on the test data
    3- Populates the metrics' lists with their respective values 
    4- Plots data and the regression model.

    @type  model: type of the model we'll be using
    @param model: the model itself.
    @type  tuned: boolean
    @param tuned: Specifies if the models have been hyperparameter tuned or not
    @rtype:   Void
    @return:  None.
    """
  # Initiate lists
  error_mean_square = []
  error_mean_absolute = []
  root_mean_square = []

  r2_metric = []
  Model_name = []
  # Train the estimator
  model.fit(X_train_new, y_train)
  # Test the estimator
  y_predict = pd.DataFrame(model.predict(X_test_new), columns = ['Predicted Output'])
  # Populate the Lists which we'll use later to create a results dataframe
  error_mean_square.append(int(mean_squared_error(y_predict, y_test)))
  error_mean_absolute.append(int(mean_absolute_error(y_predict, y_test)))
  root_mean_square.append(math.sqrt(int(mean_squared_error(y_predict, y_test))))
  r2_metric.append(r2_score(y_predict, y_test))
  Model_name.append("tuned " + type(model).__name__ if tuned else type(model).__name__)
  # Plot the data and a linear regression fit for each estimator
  plt.figure(figsize = (19, 6))
  sns.regplot(data = pd.concat([y_predict, y_test.to_frame().reset_index(drop = True)],
                axis = 1, ignore_index = False),
                y = 'Predicted Output', x = 'selling_price',
                color = None, marker = 'o')
  plt.title("Comparision of predicted values and the actual values for "+type(model).__name__, fontsize = 13)
  plt.show()
  print('\n')
  results_data = {'Models': Model_name,
        'Mean Square Error': error_mean_square,
        'Root Mean Square Error' : root_mean_square,
        'Mean Absolute Error': error_mean_absolute,
        'R2 score':r2_metric,
        }
  importances = model.feature_importances_
  #
  # Sort the feature importance in descending order
  #
  sorted_indices = np.argsort(importances)[::-1]
  plt.title('Feature Importance')
  plt.bar(range(X_train.shape[1]), importances[sorted_indices], align='center')
  plt.xticks(range(X_train.shape[1]), X_train.columns[sorted_indices], rotation=90)
  plt.tight_layout()
  plt.show()
  return results_data

def Barplot_results(metric, data):
  """
    this function makes a comparative plot between all the estimators for a given metric.

    @type  metric: String
    @param metric: List of the models we'll use for regression over the training data.

    @type  data: Pandas Dataframe
    @param data: Dataframe where the first column are model names and the rest are metrics.

    @rtype:   Void
    @return:  None.
    """
  plt.figure(figsize = (19, 6))
  splot = sns.barplot(data = data, x = 'Models', y = metric, palette = 'Paired')
  for p in splot.patches:
      splot.annotate(format(p.get_height(), '.0f'), 
                     (p.get_x() + p.get_width() / 2., p.get_height()), 
                     ha = 'center', va = 'center', 
                     xytext = (0, 9), 
                     textcoords = 'offset points')
  plt.xticks(fontsize = 13)
  plt.yticks(fontsize = 13)
  plt.title("Barplot of various machine learning regression models with " + metric, fontsize = 20)
  #plt.savefig('machine_learning_models_outcomes.png')
  plt.show()
  print('\n')

"""#Reading the csv file"""

#Reading the CSV file from my drive 
cars_df = pd.read_csv("/content/drive/MyDrive/AQSONE_Practice_Test_DS_V3_Dataset_A - AQSONE_Practice_Test_DS_V3_Dataset_A.csv")
cars_df.head()

"""Creating a backup"""

backup=cars_df.copy()

cars_df.shape

cars_df.info()

"""I see that most of these columns are of type string, the engine however seems to have a combination of number and a "CC""""

cars_df['engine']

"""We will slice the CC away in later processing steps to keep just the number

Preliminary step: 

reading the names of each car we realise that it 
generaly starts with the make(brand/company) of the car, this information can be more powerful prediction wise than the name that has a lot of noise (the model of the car has a lot of extra variability that we can't take advantage of)
"""

cars_df.insert(1, 'make', cars_df["name"].map(lambda x :x.split()[0]))

"""<a id='EDA'></a>

# Exploratory Data Analysis (EDA)

It is now time to perform exploratory data analysis for the data to get interesting insights from data. Below are some code cells along with their plots which give us good insights into our data.
"""

plt.figure(figsize = (20, 15))
sns.countplot(y = cars_df.make)
plt.title("number of cars for each make", fontsize = 20)
plt.show()

"""Taking a look at this plot shows that Maruti manufacturer has the highest number of cars, Followed by Hyundai, this could have an effect on lowering the average price of the cars. """

plt.figure(figsize = (20, 15))
sns.countplot(cars_df.year, palette = 'viridis')
plt.title("Number of cars in each year", fontsize = 20)
plt.show()

"""As could be seen from the plot, our data accounts for a lot of recent cars however we do have a lot of cars that are slightly older (>5 years). we need to check the relationship between car age and price to have a better image of this feature """

plt.figure(figsize = (10, 5))
sns.countplot(cars_df.seats, palette = 'viridis')
plt.title("Number of seats in the cars", fontsize = 20)
plt.show()

"""Most cars have the same number of seats 5, this is bad in terms of predictive value, since the number of the seats won't fluctuate as much when the price changes

## Null values
"""

msno.matrix(cars_df, color = (0.5, 0.5, 0.5))

"""Missingno is a useful machine learning plotting library used for understanding the total number of missing values in the data. We see that *seats* and *engine* features have a lot of missing values as denoted by the white strips"""

# checking the number of missing values
cars_df[['engine','seats']].isnull().sum()

"""The number of missing data points is very small compared to the dataset, plus as we've seen in the *Missingno* figure, the records that include the missing values for one feature are exactly the ones for the other feature, which means we can opt out for deleting these records and we'll only be losing exactly 221 (instead of the worst case of 442 if the records were different for each feature )

## Checking relationship of Different variables with Price

### Checking relationship of year with Price
"""

plt.figure(figsize = (20, 10))
cars_df.groupby('year')['selling_price'].mean().plot(kind = 'bar', color = 'g')
plt.title("The Average Price of cars in different years", fontsize = 20)
plt.show()

"""*   I grouped the cars based on the years of manufacture and then took the average price of the cars after grouping them. We see that as the years progress, there is a steady average increase in the prices of cars from our data respectively. 
*   2020 is a bit of an "outlier" year in this dataset, it doesn't have enough cars as seen in the number of cars figure (perhaps this data dates from the first quarter of 2020), yet the average price was almost as high as 2019 which had 12 times the number of cars!

### Checking relationship of transmission with Price
"""

plt.figure(figsize = (10, 10))
cars_df.groupby('transmission')['selling_price'].mean().plot(kind = 'bar', color = 'y')
plt.title("The Average Price of cars in different tranmission types", fontsize = 20)
plt.show()

cars_df.transmission.value_counts()

"""The automatic cars have a much higher average selling price than manual cars.

### Checking relationship of make with Price
"""

plt.figure(figsize = (20, 15))
cars_df.groupby(['make']).mean()['selling_price'].sort_values(ascending = False).plot(kind = 'bar', fontsize = 15, color = 'black')
plt.title("The average price of cars of different makes", fontsize = 20)
plt.show()

"""We notice that some of the makes with the lowest number of cars in the dataset, have the highest average selling price. this is in line with our expectations of luxury cars and their prices. Maruti on the other hand has a much lower average selling price, to the point where we can hypothetize that the more cars of a certain brand there is, the cheaper they are (on average)

### Checking relationship of seller_type with Price
"""

plt.figure(figsize = (10, 5))
cars_df.groupby(['seller_type']).mean()['selling_price'].sort_values(ascending = False).plot(kind = 'bar', fontsize = 15, color = 'black')
plt.title("The average price of cars of different seller types", fontsize = 20)
plt.show()

"""Dealers tend to sell with the highest prices

### Checking relationship of owner with Price
"""

plt.figure(figsize = (15, 10))
cars_df.groupby(['owner']).mean()['selling_price'].sort_values(ascending = False).plot(kind = 'bar', fontsize = 15, color = 'black')
plt.title("The average price of cars of different owner types", fontsize = 20)
plt.show()

"""the more a car is passed down from different owners the less it is values

### Checking relationship of fuel with Price
"""

plt.figure(figsize = (15, 10))
cars_df.groupby(['fuel']).mean()['selling_price'].sort_values(ascending = False).plot(kind = 'bar', fontsize = 15, color = 'black')
plt.title("The average price of cars of different fuels", fontsize = 20)
plt.show()

"""Diesel cars are pricier than other types of fuel

### Checking km_driven of fuel with Price
"""

sns.relplot(x='km_driven',y='selling_price',data=cars_df,height=7,aspect=1.5)

"""the relationship here is a bit skewed, but we can still see that cars with very low milage have a higher selling price than others.

### setting numerical and categorical columns variables
"""

numerical_columns = list(cars_df._get_numeric_data().columns)
categorical_columns = list(set(cars_df.columns) - set(numerical_columns))

plt.figure(figsize = (15, 15))
heatmap_data = cars_df[numerical_columns].corr()

sns.heatmap(heatmap_data, cmap = 'BuPu', annot = True)

"""#Data preprocessing

Remove the CC at the end of the engine values
"""

cars_df['engine'] = cars_df["engine"].map(lambda x : str(x).split()[0])

"""##Deleting rows with null values"""

cars_df= cars_df.dropna().reset_index(drop=True)

cars_df.shape

# checking the number of missing values
cars_df[['engine','seats']].isnull().sum()

"""It is also time to create useful features that will aid in predicting car prices. One feature is the *age* of the car. If the car is manufactured for a long time, the price would be influenced by this feature. As you can see in the above code snippet the ‘Years of Manufacture’ feature is created and assigned to the data."""

cars_df['Present Year'] = 2021   # seeing as the data ends in 2020
cars_df['car_age'] = cars_df['Present Year'] - cars_df['year']
cars_df.drop(['Present Year'], inplace = True, axis = 1)
cars_df.drop(['year'], inplace = True, axis = 1)

numerical_columns

numerical_columns.remove('year')

# I will drop the name as well as we already extracted the make, as I suspect the different model names will just serve as noise.
cars_df.drop(['name'], inplace = True, axis = 1)
categorical_columns.remove('name')

"""## Removing outliers"""

for col in  numerical_columns:
  clipping_outliers(cars_df, col)

cars_df.head()

cars_df.info()

"""Seems that *engine* feature is still of type string so let's take care of that."""

cars_df['engine'] = pd.to_numeric(cars_df['engine'])

cars_df.info()

# Update the categorical columns list
categorical_columns.remove('engine')

"""## Splitting data into training and test sets"""

X = cars_df.drop(['selling_price'],axis=1)
y = cars_df['selling_price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 100)

"""## encoding categorical features"""

encoder = TargetEncoder(cols = 'make')
encoder.fit(X_train['make'], y_train.to_frame()['selling_price'])
X_train['make'] = encoder.transform(X_train['make'])
X_test['make'] = encoder.transform(X_test['make'])
categorical_columns.remove('make')

encoder = OneHotEncoder()
encoder.fit(X_train[categorical_columns])
one_hot_encoded_output_train = encoder.transform(X_train[categorical_columns])
one_hot_encoded_output_test = encoder.transform(X_test[categorical_columns])

one_hot_encoded_output_train.head()

X_train = X_train.drop(categorical_columns,axis =1)
X_test = X_test.drop(categorical_columns,axis =1)

X_train = pd.concat([X_train, one_hot_encoded_output_train], axis =1)
X_test = pd.concat([X_test, one_hot_encoded_output_test], axis =1)

X_train.info()

"""## Feature Scaling"""

scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_new = scaler.transform(X_train)
X_test_new = scaler.transform(X_test)

X_train_new[0]

"""Our data is ready for fitting.

#Model building

For these next models I tried to go from least complext to more complex models.
"""

Models= [LinearRegression(),
         SVR(),
         KNeighborsRegressor(n_neighbors = 2),
         DecisionTreeRegressor(splitter = 'random'),
         GradientBoostingRegressor(),
         RandomForestRegressor()]

results_dict = try_regressors(Models)

results_dataframe = pd.DataFrame(results_dict).sort_values(by=['Root Mean Square Error'])
results_dataframe

metrics = results_dataframe.iloc[:,1:4].columns
for metric in metrics:
  Barplot_results(metric,results_dataframe)

"""The random forest regressor performed the best out of the 6 algorithms we used

# hyperparameter tuning

Hyperparameter tuning using RandomizedSearch CV
"""

n_estimators = [5,20,50,100] # number of trees in the random forest
max_features = ['auto', 'sqrt'] # number of features in consideration at every split
max_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree
min_samples_split = [2, 6, 10] # minimum sample number to split a node
min_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node
bootstrap = [True, False] # method used to sample data points

random_grid = {'n_estimators': n_estimators,

'max_features': max_features,

'max_depth': max_depth,

'min_samples_split': min_samples_split,

'min_samples_leaf': min_samples_leaf,

'bootstrap': bootstrap}

#rf = RandomForestRegressor()
#rf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,
#               n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = -1)

#rf_random.fit(X_train_new, y_train)

#print ('Random grid: ', random_grid, '\n')
# print the best parameters
#print ('Best Parameters: ', rf_random.best_params_, ' \n')

model = [RandomForestRegressor(n_estimators =  100, min_samples_split = 2, min_samples_leaf = 1, max_features= 'sqrt', max_depth= 120, bootstrap= True)]
tuned_rf = try_regressors(model, tuned = True)

tunedrf_df = pd.DataFrame(tuned_rf)
results_dataframe = pd.concat([results_dataframe,tunedrf_df], ignore_index=True)
results_dataframe

for metric in metrics:
  Barplot_results(metric,results_dataframe)

"""our best model is still the Random Forest Regressor

one issue I've had with these metrics is that they are scaled to the unit of the target variable, and I would personally like to have a metric that is slightly smaller for a clear difference between models. which is why I want to introduce the MAPE.

## Update my function
"""

def try_regressors2(models, tuned = False):
  """
    this function:
    1- Trains the estimators inside the models list
    2- Creates a dataframe that includes the prediction of each model on the test data
    3- Populates the metrics' lists with their respective values 
    4- Plots data and a linear regression model fit for each model.

    @type  models: Python List
    @param models: List of the models we'll use for regression over the training data.
    @type  tuned: boolean
    @param tuned: Specifies if the models have been hyperparameter tuned or not
    @rtype:   Void
    @return:  None.
    """
  # Initiate lists
  Mean_Absolute_Percentage_Error = []
  root_mean_square = []
  r2_metric = []
  Model_name = []
  # Generate colors 
  colors = generate_colors(len(models))
  # Loop over the estimators
  for (model,color) in zip(models,colors):
    # Train the estimator
    model.fit(X_train_new, y_train)
    # Test the estimator
    y_predict = pd.DataFrame(model.predict(X_test_new), columns = ['Predicted Output'])
    # Populate the Lists which we'll use later to create a results dataframe
    Mean_Absolute_Percentage_Error.append(mean_absolute_percentage_error(y_test,y_predict))
    root_mean_square.append(math.sqrt(int(mean_squared_error(y_predict, y_test))))
    r2_metric.append(r2_score(y_predict, y_test))
    Model_name.append("tuned " + type(model).__name__ if tuned else type(model).__name__)
    # Plot the data and a linear regression fit for each estimator
    plt.figure(figsize = (19, 6))
    sns.regplot(data = pd.concat([y_predict, y_test.to_frame().reset_index(drop = True)],
                axis = 1, ignore_index = False),
                y = 'Predicted Output', x = 'selling_price',
                color = color, marker = 'o')
    plt.title("Comparision of predicted values and the actual values for "+type(model).__name__, fontsize = 13)
    plt.show()
    print('\n')
  results_data = {'Models': Model_name,
         'Mean Absolute Percentage Error' : Mean_Absolute_Percentage_Error,         
        'Root Mean Square Error' : root_mean_square,
        'R2 score':r2_metric
        } 
  return results_data

Models= [LinearRegression(),
         SVR(),
         KNeighborsRegressor(n_neighbors = 2),
         DecisionTreeRegressor(splitter = 'random'),
         GradientBoostingRegressor(),
         RandomForestRegressor(n_estimators =  100, min_samples_split = 2, min_samples_leaf = 1, max_features= 'sqrt', max_depth= 120, bootstrap= True)]

results_data_with_MAPE =  try_regressors2(Models)

results_dataframe_with_MAPE = pd.DataFrame(results_data_with_MAPE).sort_values(by=['Mean Absolute Percentage Error'])
results_dataframe_with_MAPE

"""# Results and feature importance"""

rf_data = try_one_regressor(RandomForestRegressor(n_estimators =  100, min_samples_split = 2, min_samples_leaf = 1, max_features= 'sqrt', max_depth= 120, bootstrap= True))

"""We notice here that some features have rarely any importance in the regression, this is an indicator to use some type of dimensionality reduction technique such as PCA, UMAP or T-SNE"""

results_dataframe_with_MAPE.iloc[0]

results_dataframe.iloc[0]

"""**Observations**

1.   The models almost all do very well on the low priced cars, but tend to act 
poorly on the high priced cars, this makes sense given the frequencies in our distribution (most cars in the dataset are not expensive)
2.   the make, engine and car_age are the most significatif features in the dataset, and seats seems to be irrelevent

**Improvments**


1.  We could use some method of dimentionality reduction after the data preprocessing step, the one hot encoder tends to leave some collinearity between variables (example: if it's not seller type one or two then it's definetly 3)
2.  Getting some data with a distribution that leans towards the high range of cars ( luxury high priced cars) would help to predict better their prices
3.   Testing the robustness of our alogirthms is very important to do in later steps, as well as checking for overfitting
"""